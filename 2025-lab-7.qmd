---
title: "Lab 7 - Causality: DAGs"
subtitle: "BSMM 8740 Fall 2024"
author: "110191306 - Mufid Hossain"
format: html
editor: visual
self-contained: true
---

## Introduction

In today's lab, you'll practice working with

-   Potential Outcomes and causal estimands,
-   Building DAGs and extracting adjustment sets,
-   How to achieve exchangeability through RCT and IPW,
-   The FWL theorem to understand the structure of regression,
-   Causal worklows,
-   and Instrumental Variables.

By the end of the lab you will...

-   Be able to build DAGs to model causal assumptions and use the causal model to answering causal questions.
-   Be able to build a causal workflow for real world causal problems.Getting started

## Getting started

-   To complete the lab, log on to **your** github account and then go to the class [GitHub organization](https://github.com/bsmm-8740-fall-2024) and find the **2024-lab-7-\[your github username\]** repository .

    Create an R project using your **2024-lab-7-\[your github username\]** repository (remember to create a PAT, etc.) and add your answers by editing the `2024-lab-7.qmd` file in your repository.

-   When you are done, be sure to: **save** your document, **stage**, **commit** and [**push**]{.underline} your work.

::: callout-important
To access Github from the lab, you will need to make sure you are logged in as follows:

-   username: **.\\daladmin**
-   password: **Business507!**

Remember to (create a PAT and set your git credentials)

-   create your PAT using `usethis::create_github_token()` ,
-   store your PAT with `gitcreds::gitcreds_set()` ,
-   set your username and email with
    -   `usethis::use_git_config( user.name = ___, user.email = ___)`
:::

## Packages

```{r}
#| message: false
# check if 'librarian' is installed and if not, install it
if (! "librarian" %in% rownames(installed.packages()) ){
  install.packages("librarian")
}
  
# load packages if not already loaded
librarian::shelf(
  tidyverse, broom, rsample, ggdag, causaldata, halfmoon, ggokabeito, malcolmbarrett/causalworkshop
  , magrittr, ggplot2, estimatr, Formula, r-causal/propensity, gt, gtExtras, patchwork)

# set the default theme for plotting
theme_set(theme_bw(base_size = 18) + theme(legend.position = "top"))
```

::: callout-warning
## Package installs

If you install packages outside of the ones below, [**remove**]{.underline} your `install.packages(.)` code [**before submitting your solutions**]{.underline}.

Leaving instructions that will install packages may cause your code to fail, but more importantly, it can corrupt the TA/instructor's system.
:::

## Exercise 1: ATT, ATU, ATE

Open the spreadsheet at **data/outcomes.xlsx**. This spreadsheet contains potential outcomes due to treatment for 10 individuals, including their treatment status.

Fill out the treatment effect and outcome columns in the spreadsheet, and then compute the ATT, ATT, and ATU

::: {#Q1 .callout-note appearance="simple" icon="false"}
## YOUR ANSWER Q1:

random treatment:

-   ATT is **0.4**
-   ATU is **-1.0**
-   ATE is **-0.3**

non-random treatment:

-   ATT is **2.67**
-   ATU is **-4.75**
-   ATE is **-0.3**
:::

## Exercise 2: DAGs and open paths

Find the open paths from D (treatment) to Y (outcome) in the four DAGs below.

You can examine the DAGS to identify the open paths (this is the recommended first step) and then use the code for the DAGs (below), along with the function `dagitty::paths` to confirm.

```{r}
#| echo: false
# (1)
dag_1 <- 
ggdag::dagify(
  A ~ D
  , Y ~ A
  , B ~ A
  , coords = ggdag::time_ordered_coords(
    list(
      "D"             # time point 1
      , c("A","B")    # time point 2
      , "Y"           # time point 3
    )
  ),
  exposure = "D",
  outcome = "Y"
) 

# (2)
dag_2 <- 
ggdag::dagify(
  D ~ A
  , E ~ D
  , Y ~ E
  , E ~ F
  , B ~ F
  , B ~ A
  , C ~ B
  , coords = ggdag::time_ordered_coords(
    list(
      c("A","D")             # time point 1
      , c("B","F","E")    # time point 2
      , c("C","Y")           # time point 3
    )
  ),
  exposure = "D",
  outcome = "Y"
) 

# (3)
dag_3 <- 
ggdag::dagify(
  A ~ D
  , Y ~ D
  , D ~ B
  , A ~ B
  , Y ~ B
  , coords = ggdag::time_ordered_coords(
    list(
      "D"             # time point 1
      , c("B","A")    # time point 2
      , "Y"           # time point 3
    )
  ),
  exposure = "D",
  outcome = "Y"
) 

# (4)
dag_4 <- 
  ggdag::dagify(
    Y ~ D
    , Y ~ C
    , D ~ B
    , D ~ A
    , B ~ C
    , B ~ A
    , coords = ggdag::time_ordered_coords(
      list(
        c("A","D")             # time point 1
        , "B"    # time point 2
        , c("C","Y")           # time point 3
      )
    ),
    exposure = "D",
    outcome = "Y"
  ) 
# %>% 
#   ggdag::ggdag(text = TRUE) +
#   ggdag::theme_dag()

dag_flows <- 
  purrr::map(
    list(dag_1 = dag_1, dag_2 = dag_2, dag_3 = dag_3, dag_4 = dag_4)
    , ggdag::tidy_dagitty
  ) |> 
  purrr::map("data") |> 
  purrr::list_rbind(names_to = "dag") |> 
  dplyr::mutate(dag = factor(dag, levels = c("dag_1", "dag_2", "dag_3", "dag_4")))

dag_flows |> 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  ggdag::geom_dag_edges(edge_width = 1) + 
  ggdag::geom_dag_point() + 
  ggdag::geom_dag_text() + 
  facet_wrap(~ dag) +
  ggdag::expand_plot(
    expand_x = expansion(c(0.2, 0.2)),
    expand_y = expansion(c(0.2, 0.2))
  ) +
  ggdag::theme_dag()
```

::: {#Q2 .callout-note appearance="simple" icon="false"}
## YOUR ANSWER Q2:

-   DAG_1
    -   open paths from D (treatment) to Y (outcome):
        -   **D-\>A-\>Y (Causal path)**
-   DAG_2
    -   open paths from D (treatment) to Y (outcome):
        -   **D-\>E-\>Y (Causal path)**
-   DAG_3
    -   open paths from D (treatment) to Y (outcome):
        -   **D-\> Y (Causal path)**
        -   **D \<-B -\> Y (Open backdoor path through B)**
-   DAG_4
    -   open paths from D (treatment) to Y (outcome):
        -   **D-\> Y (Causal path)**
        -   **D\<-B\<-C-\>Y (Open backdoor path through B and C)**
:::

## Exercise 3: Building a DAG

You work for a company that sells a commodity to retail customers, and your management is interested in the relationship between your **price** and the **demand** for the commodity at your outlets. You have one competitor and your pricing tactic is to set your price at slightly less that your competitor's. Your company surveys the competitor's prices several times per day and once you know the competitor's price, the pricing team resets your prices according to the pricing tactic. The public is well informed of both prices when they make their choice to buy.

You and your competitor buy from the wholesaler at a price that is set by the global market, and the wholesaler's price is reset at the beginning of the each day according to the market price at the end of the day before. As the market is traded globally it reflects global demand for the commodity as well as other global and local economic shocks that you customers might be exposed to (interest rates, general business conditions, wages, etc.).

Your company has panel data (i.e time-stamped) on local economic conditions, its own sales, its own prices, competitor prices and wholesale prices, and has asked you to do an analysis of the pricing tactics to increase demand.

1.  To confirm your understanding of the business, perhaps identify missing data, and to inform your analysis, create a DAG describing the assumed relationships between the driving factors for this problem.

2.  What data might be missing from dataset provided by the company?

::: {#Q3 .callout-note appearance="simple" icon="false"}
## YOUR ANSWER Q3:

Finish constructing the DAG and use this code to display it

```{r}
#| eval: false
#| label: build a DAG to represent a demand model with price as treatment
set.seed(8740)
# fill in the missing node(s)
ggdag::dagify(
  demand ~ price + c_price + economy           # demand for the commodity at your business
  , price ~ c_price + wholesale           # price for the commodity at your business
  , c_price ~ wholesale + economy       # competitor price for the commodity 
  , wholesale ~ economy      # wholesale price for all businesses
  , economy ~ 1       # a measure of general economic activity
  , exposure = "price"
  , outcome = "demand"
  , labels = c(
      demand = "demand",
      price = "price",
      c_price = "c_price",
      economy = "economy",
      wholesale = "wholesale"
    )
) %>%
  ggdag::ggdag(use_labels = "label", text = FALSE) +
  ggdag::theme_dag()
```

What data might be missing from the company data / this DAG, if any: **The dataset may be missing global market price data, customer-specific factors like income or preferences, and marketing or promotional variables that also influence demand and pricing.**
:::

## Exercise 4: Inverse Probability Weights (IPWs)

in class we used the function `propensity::wt_ate` to calculate inverse probability weights, starting from the propensity scores, as in the code below:

```{r}
#| eval: true
#| label: build a model for the probability of treatment (propensity)
propensity_model <- glm(
  net ~ income + health + temperature,
  data = causalworkshop::net_data,
  family = binomial()
)
```

Repeat the calculation of the IPWs, using the definition of the weight as the inverse probability:

::: {#Q4 .callout-note appearance="simple" icon="false"}
## YOUR ANSWER Q4:

```{r}
#| eval: false
#| label:  calculate inverse probability weights using the propensity model
#
# calculate inverse probability weights
net_data_wts <- propensity_model |>
  broom::augment(newdata = causalworkshop::net_data, type.predict = "response") |>
  dplyr::mutate(
    # manual IPW for ATE: w = D/p + (1-D)/(1-p)
    ip_wts = (as.integer(net) / .fitted) + ((1 - as.integer(net)) / (1 - .fitted))
  )

```

and show that your calculated weights are the same as those computed by `propensity::wt_ate`

```{r}
#| label: compare your weights with those of the function wt_ate in the propensity
# show that ip_wts = wts

net_data_wts <- net_data_wts |>
  dplyr::mutate(wts_pkg = propensity::wt_ate(.fitted, as.integer(net)))

all.equal(net_data_wts$ip_wts, as.numeric(net_data_wts$wts_pkg), tolerance = 1e-12)
max(abs(net_data_wts$ip_wts - as.numeric(net_data_wts$wts_pkg)))

```
:::

## Exercise 5: Randomized Controlled Trials

The essence of exchangeability is that the treated and untreated groups are very similar with respect to values of potential confounders. Randomization of treatment makes outcomes independent of treatment, and also makes the treated and untreated groups very similar with respect to values of potential confounders.

Show that this is the case for our mosquito net data by simulating random treatment assignment as follows:

```{r}
#| echo: true
#| label: simulate random treatment asignment
#
# use this data - mosquito net data plus a row id numer
smpl_dat <- causalworkshop::net_data |>
  tibble::rowid_to_column()
```

1.  use `tidysmd::tidy_smd` with `smpl_dat` and group=net to calculate the standardized mean differences (SMDs) for the confounders income, health and temperature.
2.  use `dplyr::slice_sample` to randomly sample from `smpl_dat`, with proportion 0.5. Give this sample data a name.
3.  mutate the sample to add a column with smpl = 1.
4.  take the data not in the first sample and form a second sample (start with the original data (smpl_dat) and remove the rows that appear in the sample of step 1. This is why we added a row id. Give this second sample data a name.
5.  mutate the second sample to add a column with smpl = 0.
6.  bind the two samples together by rows (e.g. `dplyr::bind_rows`).
7.  use `tidysmd::tidy_smd` with the combined samples from step 6 and group=smpl to calculate the standardized mean differences (SMDs) for the confounders income, health and temperature.

Did randomization make the treatment groups more alike with respect to income, health and temperature?

::: {#Q5 .callout-note appearance="simple" icon="false"}
## YOUR ANSWER Q5:

```{r}
#| label: compare treatment and control group covariates under randomization
# show the smd table for the original data and for the randomized data

# Step 1: Calculate standardized mean differences (SMDs) for the original treatment groups
# This shows how different the treated and untreated groups are before randomization
tidysmd::tidy_smd(
  .df = smpl_dat,
  .vars = c(income, health, temperature),
  .group = net
)


# Step 2: Set seed for reproducibility and randomly sample 50% of the data
# This will be our "treated" group in the randomized experiment
set.seed(8740)
sample1 <- smpl_dat |>
  dplyr::slice_sample(prop = 0.5)

# Step 3: Add a column to sample1 to indicate it belongs to the randomized treatment group
sample1 <- sample1 |>
  dplyr::mutate(smpl = 1)

# Step 4: Create the second sample by removing rows that were selected in sample1
# This will be our "control" group in the randomized experiment
sample2 <- smpl_dat |>
  dplyr::filter(!rowid %in% sample1$rowid)

# Step 5: Add a column to sample2 to indicate it belongs to the randomized control group
sample2 <- sample2 |>
  dplyr::mutate(smpl = 0)

# Step 6: Combine both samples into one dataset
combined <- dplyr::bind_rows(sample1, sample2)

# Step 7: Calculate SMDs again to assess balance after randomization
# If randomization worked well, these values should be close to 0
tidysmd::tidy_smd(
  .df = combined,
  .vars = c(income, health, temperature),
  .group = smpl
)


#Did randomization make the treatment groups more alike with respect to income, health and temperature? - Yes. Before randomization, the standardized mean differences (SMDs) for income and health were large (-0.33 and -0.25), indicating imbalance. After randomization, the SMDs for all three variables dropped close to zero (e.g., income ≈ 0, health ≈ 0.019, temperature ≈ -0.005), showing that randomization made the groups much more similar with respect to these confounders.
```
:::

## Exercise 6: Frisch-Waugh-Lovell Theorem

Here we'll look at credit and default-risk data. First we'll load the data:

```{r}
#| label: load data
#
# load data
risk_data = readr::read_csv("data/risk_data.csv", show_col_types = FALSE)
```

The FWL Theorem states that a multivariate linear regression can be estimated all at once or in three separate steps. For example, you can regress `default` on the financial variables `credit_limit`, `wage`, `credit_score1`, and `credit_score2` as follows:

```{r}
#| label: perform regression (outcome on covariates and treatment)
#
# regress default on financila variabbles in the dataset
model <- lm(default ~ credit_limit + wage +  credit_score1 + credit_score2, data = risk_data)

model |> broom::tidy(conf.int = TRUE)
```

Per FWL you can also break this down into

1.  a [**de-biasing**]{.underline} step, where you regress the treatment (`credit_limit`) on the financial confounders `wage`, `credit_score1`, and `credit_score2` , obtaining the residuals
2.  a [**de-noising**]{.underline} step, where you regress the outcome (`default`) on the financial confounders, obtaining the residuals
3.  an [**outcome model**]{.underline}, where you regress the outcome residuals from step 2 on the treatment residuals of step 1.

Due to confounding, the data looks like this, with default percentage trending down by credit limit.

```{r}
#| echo: true
#| label: show data
risk_data |> 
  dplyr::group_by(credit_limit) |> 
  # add columns for number of measurements in the group, and the mean of the group
  dplyr::mutate(size = n(), default = mean(default), ) |> 
  # pull ot the distict values
  dplyr::distinct(default,credit_limit, size) |> 
  ggplot(aes(x = credit_limit, y = default, size = size)) +
  geom_point() +
  labs(title = "Default Rate by Credit Limit", x = "credit_limit", y = "default") +
  theme_minimal()

```

[**Step 1:**]{.underline}

-   Create the debiasing model, and
-   add the residuals to the risk_data, saving the result in `risk_data_deb` in a column `credit_limit_res`.
-   plot the de-biased data
-   regress the outcome (`default`) on `credit_limit_res`

The de-biasing step is crucial for estimating the correct causal effect, while the de-noising step is nice to have, since it reduces the variance of the coefficient estimate and narrows the confidence interval.

[**Step 2:**]{.underline}

-   create the de-noising model,
-   add the residuals to the de-biased data (risk_data_deb), saving the result in `risk_data_denoise` in a column `default_res`.

[**Step 3:**]{.underline}

-   regress the default residuals (`default_res`) on the credit limit residuals (`credit_limit_res`)

::: {#Q6 .callout-note appearance="simple" icon="false"}
## YOUR ANSWER Q6:

```{r}
#| eval: false
#| label: perform the first stage or de-biasing regression (STEP 1)
# create de-biasing model
debiasing_model <- lm(credit_limit ~ wage + credit_score1 + credit_score2, data = risk_data)

debiasing_model |> broom::tidy(conf.int = TRUE)


# add a column with the residuals of the de-biasing model 
# (add the residuals to the mean credit limit to give a nice interpretation to a zero residual)
risk_data_deb <- risk_data |> 
  dplyr::mutate(credit_limit_res = mean(credit_limit) + debiasing_model$residuals)


risk_data_deb |> 
  # round the residuals prior to grouping
  dplyr::mutate(credit_limit_res = round(credit_limit_res,digits=-2)) |> 
  dplyr::group_by(credit_limit_res) |> 
  # add columns for number of measurements in the group, and the mean of the group
  dplyr::mutate(size = n(), default = mean(default), ) |> 
  # only plot the residual groups with 'large' numbers of cases
  dplyr::filter(size>30) |> 
  # pull ot the distict values
  dplyr::distinct(default,credit_limit_res, size) |> 
  ggplot(aes(x = credit_limit_res, y = default, size = size)) +
  geom_point() +
  labs(title = "Default Rate by Debiased Credit Limit", x = "credit_limit", y = "default")+
  theme_minimal()

# run regression with residuals
lm(default ~ credit_limit_res, data = risk_data_deb) |> 
    broom::tidy(conf.int = TRUE)
```

In this last regression, the coefficient estimated for `credit_limit_res` should be the same as the coefficient estimated for `credit_limit` in the initial regression.

Note the difference in the confidence intervals though.

```{r}
#| eval: false
#| label: perform the second stage or de-noising regression (STEP 2)
# create the de-noising model
denoising_model <- 
    lm(default ~ wage + credit_score1 + credit_score2, data = risk_data_deb)


risk_data_denoise <-  risk_data_deb |> 
  # add a column with the residuals of the de-noised model 
  # (add the residuals to the mean default to give a nice interpretation to the zero residual)
  dplyr::mutate(default_res = denoising_model$residuals + mean(default))
```

Now regress the residuals on each other

```{r}
#| eval: false
#| label: regress residuals on residuals (STEP 3)
#

lm(default_res ~ credit_limit_res, data = risk_data_denoise) |> 
    broom::tidy(conf.int = TRUE)
```

How do the coefficients and confidence intervals from the FWL steps above compare to the original multivariate regression? - The coefficient for credit_limit_res from the FWL residual regression is identical to the credit_limit coefficient from the original regression, confirming the theorem.
However, the FWL regression often shows a slightly narrower confidence interval, indicating lower variance due to de-noising.
:::

## Exercise 7: Causal Modeling

### Questions at the end

In this guided exercise, we'll attempt to answer a causal question: does quitting smoking make you gain weight? Causal modeling has a special place in the history of smoking research: the studies that demonstrated that smoking causes lung cancer were observational. Thanks to other studies, we also know that, if you're already a smoker, quitting smoking reduces your risk of lung cancer. However, some have observed that former smokers tend to gain weight. Is this the result of quitting smoking, or does something else explain this effect? In the book *Causal Inference: What If* by Hernán and Robins, the authors analyze this question using several causal inference techniques.

To answer this question, we'll use causal inference methods to examine the relationship between quitting smoking and gaining weight. First, we'll draw our assumptions with a causal diagram (a directed acyclic graph, or DAG), which will guide our model. Then, we'll use a modeling approach called inverse probability weighting--one of many causal modeling techniques--to estimate the causal effect we're interested in.

We'll use data from NHEFS to try to estimate the causal effect of quitting smoking on weight game. NHEFS is a longitudinal, observational study that has many of the variables we'll need. Take a look at `causaldata::nhefs_codebook` if you want to know more about the variables in this data set. These data are included in the {causaldata} package. We'll use the `causaldata::nhefs_complete` data set, but we'll remove people who were lost to follow-up.

```{r}
#| label: load the smoking data
#
nhefs_complete_uc <- causaldata::nhefs_complete |>
  dplyr::filter(censored == 0)
nhefs_complete_uc
```

Let's look at the distribution of weight gain between the two groups.

```{r}
#| label: compare the two groups
#
nhefs_complete_uc |>
  ggplot(aes(wt82_71, fill = factor(qsmk))) + 
  geom_vline(xintercept = 0, color = "grey60", linewidth = 1) +
  geom_density(color = "white", alpha = .75, linewidth = .5) +
  ggokabeito::scale_color_okabe_ito(order = c(1, 5)) + 
  theme_minimal() +
  theme(legend.position = "bottom") + 
  labs(
    x = "change in weight (kg)",
    fill = "quit smoking (1 = yes)"
  )
```

There's a difference--former smokers do seemed to have gained a bit more weight--but there's also a lot of variation. Let's look at the numeric summaries.

```{r}
#| label: estimate the weight gain difference (the contrast)
# ~2.5 kg gained for quit vs. not quit
nhefs_complete_uc |>
  dplyr::group_by(qsmk) |>
  dplyr::summarize(
    mean_weight_change = mean(wt82_71), 
    sd = sd(wt82_71),
    .groups = "drop"
  )
```

Here, it looks like those who quit smoking gained, on average, 4.5 kg. But is there something else that could explain these results? There are many factors associated with both quitting smoking and gaining weight; could one of those factors explain away the results we're seeing here?

To truly answer this question, we need to specify a causal diagram based on domain knowledge. For most circumstances, there is no data-driven approach that consistently identify confounders. Only our causal assumptions can help us identify them. Causal diagrams are a visual expression of those assumptions linked to rigorous mathematics that allow us to understand what we need to account for in our model.

In R, we can visualize and analyze our DAGs with the `ggdag` package. `ggdag` uses `ggplot2` and `ggraph` to visualize diagrams and `dagitty` to analyze them. Let's set up our assumptions. The `dagify()` function takes formulas, much like `lm()` and friends, to express assumptions. We have two basic causal structures: the causes of quitting smoking and the causes of gaining weight. Here, we're assuming that the set of variables here affect both. Additionally, we're adding `qsmk` as a cause of `wt82_71`, which is our causal question; we also identify these as our outcome and exposure. Finally, we'll add some labels so the diagram is easier to understand. The result is a `dagitty` object, and we can transform it to a `tidy_dagitty` data set with `tidy_dagitty()`.

```{r}
#| label: creat a DAG identifying relationships between variables
# set up DAG
smk_wt_dag <- ggdag::dagify(
  # specify causes of quitting smoking and weight gain:
  qsmk ~ sex + race + age + education + 
    smokeintensity + smokeyrs + exercise + active + wt71,
  wt82_71 ~ qsmk + sex + race + age + education + 
    smokeintensity + smokeyrs + exercise + active + wt71,
  # specify causal question:
  exposure = "qsmk", 
  outcome = "wt82_71",
  coords = ggdag::time_ordered_coords(),
  # set up labels:
  # here, I'll use the same variable names as the data set, but I'll label them
  # with clearer names
  labels = c(
    # causal question
    "qsmk" = "quit\nsmoking",
    "wt82_71" = "change in\nweight",
    
    # demographics
    "age" = "age",
    "sex" = "sex",
    "race" = "race",
    "education" = "education",
    
    # health
    "wt71" = "baseline\nweight",
    "active" = "daily\nactivity\nlevel",
    "exercise" = "exercise",
    
    # smoking history
    "smokeintensity" = "smoking\nintensity",
    "smokeyrs" = "yrs of\nsmoking"
  )
) |>
  ggdag::tidy_dagitty()

smk_wt_dag
```

Let's visualize our assumptions with `ggdag()`.

```{r}
#| message: false
#| warning: false
#| label: show the DAG
smk_wt_dag |>
  ggdag::ggdag(text = FALSE, use_labels = "label")
```

What do we need to control for to estimate an unbiased effect of quitting smoking on weight gain? In many DAGs, there will be many sets of variables--called adjustment sets--that will give us the right effect (assuming our DAG is correct--a big, unverifiable assumption!). `ggdag_adjustment_set()` can help you visualize them. Here, there's only one adjustment set: we need to control for everything! While we're add it, since a {ggdag} plot is just a {ggplot2} plot, let's clean it up a bit, too.

```{r}
#| label: identify the adjustment set
smk_wt_dag |>
  ggdag::ggdag_adjustment_set(text = FALSE, use_labels = "label") +
  ggdag::theme_dag() +
  ggokabeito::scale_color_okabe_ito(order = c(1, 5)) + 
  ggokabeito::scale_fill_okabe_ito(order = c(1, 5))
```

Let's fit a model with these variables. Note that we'll fit all continuous variables with squared terms, as well, to allow them a bit of flexibility.

```{r}
#| label: fit weight gain model using adjustment set
lm(
  wt82_71~ qsmk + sex + 
    race + age + I(age^2) + education + 
    smokeintensity + I(smokeintensity^2) + 
    smokeyrs + I(smokeyrs^2) + exercise + active + 
    wt71 + I(wt71^2), 
  data = nhefs_complete_uc
) |>
  broom::tidy(conf.int = TRUE) |>
  dplyr::filter(term == "qsmk")
```

When we adjust for the variables in our DAG, we get an estimate of about 3.5 kg--people who quit smoking gained about this amount of weight. However, we are trying to answer a specific causal question: how much weight would a person gain if the quit smoking vs. if the same person did not quit smoking? Let's use an inverse probability weighting model to try to estimate that effect at the population level (what if *everyone* quit smoking vs what if *no one* quit smoking).

For a simple IPW model, we have two modeling steps. First, we fit a propensity score model, which predicts the probability that you received a treatment or exposure (here, that a participant quit smoking). We use this model to calculate inverse probability weights--1 / your probability of treatment. Then, in the second step, we use this weights in the outcome model, which estimates the effect of exposure on the outcome (here, the effect of quitting smoking on gaining weight).

For the propensity score model, we'll use logistic regression (since quitting smoking is a binary variable). The outcome is quitting smoking, and the variables in the model are all those included in our adjustment set. Then, we'll use `augment()` from {broom} (which calls `predict()` on the inside) to calculate our weights using `propensity::wt_ate()` and save it back into our data set.

```{r}
#| label: model the propensity
propensity_model <- glm(
  qsmk ~ sex + 
    race + age + I(age^2) + education + 
    smokeintensity + I(smokeintensity^2) + 
    smokeyrs + I(smokeyrs^2) + exercise + active + 
    wt71 + I(wt71^2), 
  family = binomial(), 
  data = nhefs_complete_uc
)

# calculate weights (IPW)
nhefs_complete_uc <- propensity_model |>
  # predict whether quit smoking
  broom::augment(type.predict = "response", data = nhefs_complete_uc) |>
  # calculate inverse probability
  dplyr::mutate(wts = propensity::wt_ate(.fitted, qsmk))

# select the columns needed
nhefs_complete_uc |>
  dplyr::select(qsmk, .fitted, wts)
```

Let's look at the distribution of the weights.

```{r}
#| label: look at the distribution of weights to identify non-positivity 
ggplot(nhefs_complete_uc, aes(wts)) +
  geom_histogram(color = "white", fill = "#E69F00", bins = 50) + 
  #  use a log scale for the x axis
  scale_x_log10() + 
  theme_minimal(base_size = 20) + 
  xlab("Weights")
```

It looks a little skewed, in particular there are some participants with much higher weights. There are a few techniques for dealing with this--trimming weights and stabilizing weights--but we'll keep it simple for now and just use them as is.

The main goal here is to *break* the non-causal associations between quitting smoking and gaining weight--the other paths that might distort our results. In other words, if we succeed, there should be no differences in these variables between our two groups, those who quit smoking and those who didn't. This is where randomized trials shine; you can often assume that there is no baseline differences among potential confounders between your treatment groups (of course, no study is perfect, and there's a whole set of literature on dealing with this problem in randomized trials).

Standardized mean differences (SMD) are a simple measurement of differences that work across variable types. In general, the closer to 0 we are, the better job we have done eliminating the non-causal relationships we drew in our DAG. Note that low SMDs for everything we adjust for does *not* mean that there is not something else that might confound our study. Unmeasured confounders or misspecified DAGs can still distort our effects, even if our SMDs look great!

We'll use the {halfmoon} package to calculate the SMDs, then visualize them.

```{r}
#| label: check adjustment by weighting
#
vars <- c(
  "sex", "race", "age", "education", 
  "smokeintensity", "smokeyrs", 
  "exercise", "active", "wt71"
)

# form data frame with data from smd
plot_df <- halfmoon::tidy_smd(
    nhefs_complete_uc,
    all_of(vars),
    qsmk,
    wts
)

#| plot
ggplot(
    data = plot_df,
    mapping = aes(x = abs(smd), y = variable, group = method, color = method)
) +
    halfmoon::geom_love()
```

These look pretty good! Some variables are better than others, but weighting appears to have done a much better job eliminating these differences than an unadjusted analysis.

We can also use halfmoon's `geom_mirror_histogram()` to visualize the impact that the weights are having on our population.

```{r}
#| label: show half-moon plot
nhefs_complete_uc |>
  dplyr::mutate(qsmk = factor(qsmk)) |>
  ggplot(aes(.fitted)) +
  halfmoon::geom_mirror_histogram(
    aes(group = qsmk),
    bins = 50
  ) +
  halfmoon::geom_mirror_histogram(
    aes(fill = qsmk, weight = wts),
    bins = 50,
    alpha = .5
  ) +
  scale_y_continuous(labels = abs) +
  labs(x = "propensity score") + 
  theme_minimal(base_size = 20)
```

Both groups are being *upweighted* so that their distributions of propensity scores are much more similar.

We could do more here to analyze our assumptions, but let's move on to our second step: fitting the outcome model weighted by our inverse probabilities. Some researchers call these Marginal Structural Models, in part because the model is marginal; we only need to include our outcome (`wt82_71`) and exposure (`qsmk`). The other variables aren't in the model; they are accounted for with the IPWs!

```{r}
#| label: run the weighted regression
#
ipw_model <- lm(
  wt82_71 ~ qsmk, 
  data = nhefs_complete_uc, 
  weights = wts # inverse probability weights
) 

ipw_estimate <- ipw_model |>
  broom::tidy(conf.int = TRUE) |>
  dplyr::filter(term == "qsmk")

ipw_estimate
```

This estimate is pretty similar to what we saw before, if a little smaller. In fact, for simple causal questions, this is often the case: adjusting for confounders directly in your regression model sometimes estimates the same effect as IPWs and other causal techniques. Causal techniques are special, though, in that the use counterfactual modeling, which allows you to deal with many circumstances, such as when you have selection bias or time-dependendent confounding. They also often have variance properties.

But we have other problem that we need to address. While we're just using `lm()` to estimate our IPW model, it doesn't properly account for the weights. That means our standard error is too small, which will artificially narrow confidence intervals and artificially shrink p-values. There are many ways to address this, including robust estimators. We'll focus on using the bootstrap via the {rsamples} package in this workshop, but here's one way to do it with robust standard errors:

```{r}
#| label: get robust standard errors using the estimatr package
# also see robustbase, survey, gee, and others
library(estimatr)
ipw_model_robust <- lm_robust( 
  wt82_71 ~ qsmk, 
  data = nhefs_complete_uc, 
  weights = wts 
) 

# pull the estimate
ipw_estimate_robust <- ipw_model_robust |>
  broom::tidy(conf.int = TRUE) |>
  dplyr::filter(term == "qsmk")

# show the estimate
ipw_estimate_robust
```

Now let's try the bootstrap. First, we need to wrap our model in a function so we can call it many times on our bootstrapped data. A function like this might be your instinct; however, it's not quite right.

```{r}
#| label: get robust standard errors using bootstrap
# fit ipw model for a single bootstrap sample
fit_ipw_not_quite_rightly <- function(split, ...) {
  # get bootstrapped data sample with `rsample::analysis()`
  .df <- rsample::analysis(split)
  
  # fit ipw model
  lm(wt82_71 ~ qsmk, data = .df, weights = wts) |>
    tidy()
}
```

The problem is that we need to account for the *entire* modeling process, so we need to include the first step of our analysis -- fitting the inverse probability weights.

```{r}
#| label: put entire IPW analysis within a function
#
fit_ipw <- function(split, ...) {
  .df <- rsample::analysis(split)
  
  # fit propensity score model
  propensity_model <- glm(
    qsmk ~ sex + 
      race + age + I(age^2) + education + 
      smokeintensity + I(smokeintensity^2) + 
      smokeyrs + I(smokeyrs^2) + exercise + active + 
      wt71 + I(wt71^2), 
    family = binomial(), 
    data = .df
  )
  
  # calculate inverse probability weights
  .df <- propensity_model |>
    broom::augment(type.predict = "response", data = .df) |>
    dplyr::mutate(wts = propensity::wt_ate(
      .fitted,
      qsmk, 
      exposure_type = "binary"
    ))
  
  # fit correctly bootstrapped ipw model
  lm(wt82_71 ~ qsmk, data = .df, weights = wts) |>
    tidy()
}
```

`rsample` makes the rest easy for us: `bootstraps()` resamples our data 1000 times, then we can use `purrr::map()` to apply our function to each resampled set (`splits`). `rsample`'s `int_*()` functions help us get confidence intervals for our estimate.

```{r}
#| label: put entire analysis within a function
#
# fit ipw model to bootstrapped samples | THIS MAY TAKE SOME TIME
ipw_results <- rsample::bootstraps(causaldata::nhefs_complete, 1000, apparent = TRUE) |>
  dplyr::mutate(results = purrr::map(splits, fit_ipw))

# get t-statistic-based CIs
boot_estimate <- rsample::int_t(ipw_results, results) |>
  dplyr::filter(term == "qsmk")

# show estimate
boot_estimate
```

Let's compare to our naive weighted model that just used a single estimate from `lm()`

```{r}
#| label: compare weighted model with naive model
#
dplyr::bind_rows(
  ipw_estimate |>
    dplyr::select(estimate, conf.low, conf.high) |>
    dplyr::mutate(type = "ols"),
  ipw_estimate_robust |>
    dplyr::select(estimate, conf.low, conf.high) |>
    dplyr::mutate(type = "robust"),
  boot_estimate |>
    dplyr::select(estimate = .estimate, conf.low = .lower, conf.high = .upper) |>
    dplyr::mutate(type = "bootstrap")
) |>
  #  calculate CI width to sort by it
  dplyr::mutate(width = conf.high - conf.low) |>
  dplyr::arrange(width) |>
  #  fix the order of the model types for the plot  
  dplyr::mutate(type = forcats::fct_inorder(type)) |>
  ggplot(aes(x = type, y = estimate, ymin = conf.low, ymax = conf.high)) + 
    geom_pointrange(color = "#0172B1", size = 1, fatten = 3) +
    coord_flip() +
    theme_minimal(base_size = 20) +
    theme(axis.title.y = element_blank())
```

Our bootstrapped confidence intervals are wider, which is expected; remember that they were artificially narrow in the naive OLS model!

So, we have a final estimate for our causal effect: on average, a person who quits smoking will gain 3.5 kg (95% CI 2.4 kg, 4.4 kg) versus if they had not quit smoking.

------------------------------------------------------------------------

### Questions:

-   Please enumerate the steps in the causal analysis workflow
-   What do you think? Is this estimate reliable? Did we do a good job addressing the assumptions we need to make for a causal effect, particularly that there is no confounding? How might you criticize this model, and what would you do differently?

::: {#Q7 .callout-note appearance="simple" icon="false"}
## YOUR ANSWER Q7:

# Causal workflow steps:

1.  **State the causal question** — Does quitting smoking cause weight gain?
2.  **Draw a causal diagram (DAG)** — Identify possible confounders like age, sex, exercise, smoking history, and baseline weight.
3.  **Identify adjustment set** — Use the DAG to determine which variables must be controlled to estimate the causal effect.
4.  **Fit the outcome model** — Regress weight change on quitting and all confounders.
5.  **Model the treatment (propensity)** — Predict probability of quitting smoking using logistic regression.
6.  **Compute inverse probability weights (IPW)** — Use these weights to create a pseudo-population balanced on confounders.
7.  **Fit the weighted outcome model** — Estimate causal effect of quitting smoking on weight gain.
8.  **Check balance (SMDs and plots)** — Verify that the weighting balanced confounders.
9.  **Adjust standard errors (robust / bootstrap)** — Correct for underestimated uncertainty in weighted models.
10. **Interpret the causal estimate** — Conclude how much weight gain is attributable to quitting.

# Your critique of the results of the exercise:

The estimated effect (\~3.5 kg gain) is plausible and consistent with prior research, but the estimate may not be perfectly reliable because it depends on the assumption of no unmeasured confounding. Although we adjusted for many factors (age, baseline weight, exercise, etc.), variables like diet, stress, or metabolic rate were not measured and could bias the result. The model also assumes correct functional forms (e.g., linear relationships). To improve reliability, I would: - include additional lifestyle and health covariates if available, - test for model misspecification (e.g., interactions, nonlinear effects), - and replicate using a sensitivity analysis or doubly robust estimator.
:::

## Exercise 8: Causal Modeling

An e-commerce company is studying the effect of personalized email campaigns (D) on monthly purchase amount (Y). They have data on 5,000 customers, where 2,000 received personalized emails (D=1) and 3,000 received standard emails (D=0).

**Observed data:**

-   Mean monthly purchase in personalized group: E\[Y\|D=1\] = \$120
-   Mean monthly purchase in standard group: E\[Y\|D=0\] = \$85
-   The company suspects that high-value customers were more likely to receive personalized emails

**Questions:**

-   Define the following parameters using Rubin's notation:
    -   Average Treatment Effect (ATE)
    -   Average Treatment Effect on the Treated (ATT)
    -   Average Treatment Effect on the Untreated (ATU)
-   What is the observed difference in means? What does this estimate capture?
-   If the true ATE = \$25, what is the selection bias? Show your calculation.
-   Explain why we cannot directly calculate ATT and ATU from the observed data alone.
-   Under what assumption would the observed difference in means equal the ATE? Is this assumption likely to hold in this marketing context?

::: {#Q8 .callout-note appearance="simple" icon="false"}
## YOUR ANSWER Q8:

-   Define the following parameters using Rubin's notation:

    -   Average Treatment Effect (ATE): $E[Y(1) -Y(0)]$
    -   Average Treatment Effect on the Treated (ATT): $E[Y(1)-Y(0)\mid D=1]$
    -   Average Treatment Effect on the Untreated (ATU): $E[Y(1) - Y(0) \mid D=0]$

-   What is the observed difference in means $E[Y \mid D=1] - E[Y \mid D=0] = 120 - 85 = \$35$

    -   What does this estimate capture❓ $E[Y \mid D=1] - E[Y \mid D=0] = ATE + \big( E[Y(0)\mid D=1] - E[Y(0)\mid D=0] \big)$. It captures the causal effect plus the selection bias

-   If the true ATE = \$25, what is the selection bias$\big( E[Y(0)\mid D=1] - E[Y(0)\mid D=0] \big) = E[Y \mid D=1] - E[Y \mid D=0] - ATE = 35 - 25 = \$10$

-   Explain why we cannot directly calculate ATT and ATU from the observed data alone.

    Because for each customer we observe only one potential outcome (either $Y(1)$ or $Y(0)$), never both. ATT needs $E[Y(0)\mid D=1]$ and ATU needs $E[Y(1)\mid D=0]$ — both are counterfactual without extra assumptions or designs (randomization, unconfoundedness + IPW/matching, IV, etc.).

-   Under what assumption would the observed difference in means equal the ATE? Is this assumption likely to hold in this marketing context?

    The observed difference in means equals the ATE under the unconfoundedness (ignorability) assumption, that is, when $(Y(1), Y(0)) \perp D$ which would only hold in a randomized experiment.

    In this marketing context, it’s unlikely since high-value customers were more likely to receive personalized emails.
:::

## Exercise 9: Instrumental Variables

The estimator for instrumental variable $Z$ is conceptually two regressions, one the "reduced form" and the other the "first stage"

$$
\begin{align*}
Y_{i} & =\kappa+\rho Z_{i}+\nu_{i}\;\text{the "reduced form"}\\
D_{i} & =\mu+\pi Z_{i}+\eta_{i}\;\text{the "first stage"}
\end{align*}
$$

where $\nu_1,\eta_i$ are noise terms and $\kappa,\mu$ are intercepts, and our estimate is $\beta^{IV}=\rho/\pi$.

Show that $\beta^{IV}=\frac{\text{cov}\left(Z_i,Y_i\right)}{\text{cov}\left(Z_i,D_i\right)}$

::: {#Q9 .callout-note appearance="simple" icon="false"}
## YOUR ANSWER Q9:

Please show all your work

## Derivation

From the OLS formulas for each simple regression:

$$
\hat{\rho} = \frac{\text{Cov}(Z, Y)}{\text{Var}(Z)}, \quad 
\hat{\pi} = \frac{\text{Cov}(Z, D)}{\text{Var}(Z)}.
$$

The IV (Wald) estimator is the ratio of these slopes:

$$
\hat{\beta}^{IV} 
= \frac{\hat{\rho}}{\hat{\pi}} 
= \frac{\frac{\text{Cov}(Z, Y)}{\text{Var}(Z)}}{\frac{\text{Cov}(Z, D)}{\text{Var}(Z)}} 
= \frac{\text{Cov}(Z, Y)}{\text{Cov}(Z, D)}.
$$

### Alternative Derivation (Moment Condition)

Starting from the structural model $Y = \alpha + \beta D + u$\
and assuming $\text{Cov}(Z, u) = 0$ and $\text{Cov}(Z, D) \neq 0$,\
we have the IV moment condition:

$$
E[Z(Y - \beta D)] = 0 
\;\Rightarrow\;
\text{Cov}(Z, Y) - \beta\,\text{Cov}(Z, D) = 0.
$$

Solving for $\beta$ gives:

$$
\boxed{\beta^{IV} = \frac{\text{Cov}(Z, Y)}{\text{Cov}(Z, D)}}.
$$
:::

## Exercise 10: Instrumental Variables

An **elasticity** measures the percentage change in one variable (outcome) in response to a percentage change in another variable (treatment), typically written as: $\eta = \frac{\partial{Y}/Y}{\partial{X}/X}=\frac{d\log(Y)/dY}{d\log(X)/dx}$

Common elasticities include the price elasticity of demand, but in regressing quantity on price it's very common that price and quantity are simultaneously determined, which leads to biased elasticity estimates (think of the DAG for this problem).

The following code generates synthetic e-commerce data with an elasticity of demand equal -1.5 on total price, which includes list price plus shipping cost.

Here you will use shipping costs (`shipping_cost`) as the instrument (IV), noting that it has variation (variation in shipping costs across regions and time) and shipping costs don't directly affect demand (it affects demand only through total price).

```{r}
#| echo: true
#| label: generate econ data
#| code-fold: true
#| code-summary: generate realistic looking ecommerce data
# Set seed for reproducibility
set.seed(8740)

# Generate synthetic e-commerce data
generate_ecommerce_data <- function(n = 5000) {
  # Create panel structure
  regions <- c("Northeast", "Southeast", "Midwest", "West", "Southwest")
  months <- 1:24
  
  # Expand grid for panel
  data <- tidyr::expand_grid(
    region = regions,
    month = months,
    customer_id = 1:200  # 200 customers per region-month
  ) |>
    dplyr::slice_sample(n = n) |>
    dplyr::mutate(
      # Geographic factors affecting shipping
      distance_to_hub = dplyr::case_when(
        region == "Northeast" ~ 500,
        region == "Southeast" ~ 800,
        region == "Midwest" ~ 300,
        region == "West" ~ 1200,
        region == "Southwest" ~ 900
      ),
      
      # Time-varying factors
      fuel_price = 3 + 0.5 * sin(2 * pi * month / 12) + rnorm(dplyr::n(), 0, 0.2),
      seasonal_demand = 1 + 0.3 * cos(2 * pi * month / 12),
      
      # Unobserved demand shock (creates endogeneity)
      demand_shock = rnorm(dplyr::n(), 0, 0.3),
      
      # Shipping cost (our instrument)
      shipping_cost = 5 + 0.002 * distance_to_hub + 2 * fuel_price + rnorm(dplyr::n(), 0, 1),
      
      # Product price (endogenous - responds to demand)
      base_price = 50,
      price = base_price + 5 * seasonal_demand + 3 * demand_shock + 
        0.5 * shipping_cost + rnorm(dplyr::n(), 0, 2),
      
      # Total price paid by consumer
      total_price = price + shipping_cost,
      
      # Quantity demanded (true elasticity = -1.5)
      log_quantity = 4 - 1.5 * log(total_price) + 0.5 * seasonal_demand + 
        demand_shock + rnorm(dplyr::n(), 0, 0.2),
      quantity = exp(log_quantity),
      
      # Revenue
      revenue = price * quantity,
      
      # Add customer characteristics
      income = exp(rnorm(dplyr::n(), 10.5, 0.3)),
      age = sample(18:65, dplyr::n(), replace = TRUE)
    ) |>
    # Create some missing values to make realistic
    dplyr::mutate(
      price = ifelse(runif(dplyr::n()) < 0.02, NA, price),
      quantity = ifelse(quantity < 0.1, 0.1, quantity)  # No negative quantities
    ) |>
    dplyr::filter(!is.na(price))
  
  return(data)
}

# Generate the dataset
ecom_data <- generate_ecommerce_data(n = 5000)
```

Here are some visualizations of our data. Note that the slope of demand vs total price is +ve instead of -ve.

```{r}
#| label: plots of the data
#| code-fold: true
#| code-summary: Plots of demand vs total price and total price vs shipping costs
#| message: false
#| warning: false

library(patchwork)
# Visualize price and quantity relationship
p1 <- ggplot(ecom_data, aes(x = log(total_price), y = log(quantity))) +
  geom_point(alpha = 0.5, color = "steelblue") +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(title = "Raw Price-Quantity Relationship",
       subtitle = "This relationship is biased due to endogeneity",
       x = "Log(Total Price)", y = "Log(Quantity)") +
  theme_minimal()

# Visualize instrument relevance
p2 <- ggplot(ecom_data, aes(x = shipping_cost, y = total_price)) +
  geom_point(alpha = 0.5, color = "darkgreen") +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(title = "Instrument Relevance: Shipping Cost vs Total Price",
       subtitle = "First stage relationship",
       x = "Shipping Cost", y = "Total Price") +
  theme_minimal()

p1 + p2
```

Complete the following steps in the analysis:

1.  Use ecom_data to estimate the demand elasticity of total price with a naive model, using log(quantity) \~ log(total_price) and covariates region and month (as factors), income (as log) and age.
2.  Use econ_data to model the "reduced form" stage by replacing log(total_price) with shipping_cost.
3.  Use econ_data to model the "first stage" by regressing log(total_price) on shipping_cost

::: {#Q10 .callout-note appearance="simple" icon="false"}
## YOUR ANSWER Q10:

(STEP 1) Use ecom_data to estimate the demand elasticity of total price with a naive model, using log(quantity) \~ log(total_price) and covariates region and month (as factors), income (as log) and age.

Use broom::tidy to report the coefficient of log(price) (i.e. the elasticity)

```{r}
#| eval: false
#| label: Build model and use broom::tidy to report the coefficient of log(total price) (i.e. the elasticity) 
#
# regress log demand on log total price
naive_model <- lm(
  log(quantity) ~ log(total_price) + factor(region) + factor(month) + log(income) + age,
  data = ecom_data
)

# pull the coefficient of price as the naive elasticity estimate
naive_results <- broom::tidy(naive_model, conf.int = TRUE) |>
  dplyr::filter(term == "log(total_price)")

naive_results

```

(STEP 2) use ecom_data to model the "reduced form" stage by regressing log(quantity) on shipping_cost.

Use broom::tidy to report the coefficient of shipping_cost (i.e. effect of shipping cost on demand)

```{r}
#| eval: false
#| label: Model the "reduced form" stage regressing log(demand) on shipping_cost. Use broom::tidy to extract the coefficient on shipping cost.
#
# regress log demand on shipping price
reduced_form_model <- lm(
  log(quantity) ~ shipping_cost + factor(region) + factor(month) + log(income) + age,
  data = ecom_data
)
  
# pull the coefficient of shipping cost 
reduced_form_results <- broom::tidy(reduced_form_model, conf.int = TRUE) |>
  dplyr::filter(term == "shipping_cost")


reduced_form_results
```

(STEP 3) use econ_data to model the "first stage" by regressing log(total_price) on shipping_cost

Use broom::tidy to report the coefficient of shipping_cost (i.e. effect of shipping cost on log(total_price))

```{r}
#| eval: false
#| label: use econ_data to model the "first stage" by regressing log(total_price) on shipping_cost
#
# regress log(total price)
first_stage_model <- lm(
  log(total_price) ~ shipping_cost + factor(region) + factor(month) + log(income) + age,
  data = ecom_data
)

# pull the coefficient of shipping cost in the first stage model     
first_stage_results <- broom::tidy(first_stage_model, conf.int = TRUE) |>
  dplyr::filter(term == "shipping_cost")

first_stage_results
```

(STEP 4) Estimate the true total_price elasticity

```{r}
#| eval: false
#| label: estimate total price elasticity by dividing the "reduced form" by the "first stage" coefficients
#

# estimate true elasticity
IV_elasticity_est <- reduced_form_results$estimate / first_stage_results$estimate

IV_elasticity_est

```
:::

::: render-commit-push
You're done and ready to submit your work! **Save**, **stage**, **commit**, and **push** all remaining changes. You can use the commit message "Done with Lab 7!" , and make sure you have committed and pushed all changed files to GitHub (your Git pane in RStudio should be empty) and that **all** documents are updated in your repo on GitHub.
:::

::: callout-important
## Submission

I will pull (copy) everyone's repository submissions at 5:00pm on the Sunday following class, and I will work only with these copies, so anything submitted after 5:00pm will not be graded. (**don't forget to commit and then push your work!**)
:::

## Grading

Total points available: 30 points.

![](images/rubric10.png){fig-align="center" width="600"}
